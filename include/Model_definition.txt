import torch
import torch.nn as nn
import torch.nn.functional as F

def normalize_sample(x, eps=1e-9):
    min_val = x.min(dim=2, keepdim=True)[0]
    max_val = x.max(dim=2, keepdim=True)[0]
    return (x - min_val) / (max_val - min_val + eps)

class TransformerBlock(nn.Module):
    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):
        super(TransformerBlock, self).__init__()
        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)
        self.dropout1 = nn.Dropout(dropout)
        self.norm1 = nn.LayerNorm(embed_dim)
        self.ffn = nn.Sequential(
            nn.Linear(embed_dim, ff_dim),
            nn.ReLU(),
            nn.Linear(ff_dim, embed_dim)
        )
        self.dropout2 = nn.Dropout(dropout)
        self.norm2 = nn.LayerNorm(embed_dim)

    def forward(self, x):
        attn_output, _ = self.attention(x, x, x)
        x = self.norm1(x + self.dropout1(attn_output))
        ffn_output = self.ffn(x)
        return self.norm2(x + self.dropout2(ffn_output))

class TransformerTimeModelImproved(nn.Module):
    def __init__(self, input_length=48000):
        super(TransformerTimeModelImproved, self).__init__()
        self.conv1 = nn.Conv1d(1, 32, kernel_size=5, padding=5//2)
        self.bn1 = nn.BatchNorm1d(32)
        self.relu1 = nn.ReLU()  # For fusion.
        self.pool1 = nn.MaxPool1d(4)
        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=3//2)
        self.bn2 = nn.BatchNorm1d(64)
        self.relu2 = nn.ReLU()  # For fusion.
        self.pool2 = nn.MaxPool1d(4)
        self.dropout = nn.Dropout(0.3)
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        self.fc_proj = nn.Linear(64, 128)
        self.token_dim = 16
        self.num_tokens = 8
        self.transformer1 = TransformerBlock(embed_dim=self.token_dim, num_heads=2, ff_dim=64, dropout=0.1)
        self.transformer2 = TransformerBlock(embed_dim=self.token_dim, num_heads=2, ff_dim=64, dropout=0.1)
        self.fc1 = nn.Linear(self.num_tokens * self.token_dim, 64)
        self.fc2 = nn.Linear(64, 2)

    def fuse_model(self):
        torch.quantization.fuse_modules(self, [['conv1', 'bn1', 'relu1'],
                                                ['conv2', 'bn2', 'relu2']], inplace=True)

    def forward(self, x):
        x = normalize_sample(x)
        x = self.relu1(self.bn1(self.conv1(x)))
        x = self.pool1(x)
        x = self.relu2(self.bn2(self.conv2(x)))
        x = self.pool2(x)
        x = self.dropout(x)
        x = self.global_pool(x)
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc_proj(x))
        x = x.view(-1, self.num_tokens, self.token_dim)
        x = self.transformer1(x)
        x = self.transformer2(x)
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        return self.fc2(x)