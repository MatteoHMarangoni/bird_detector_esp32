<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>guide</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
</head>
<body>
<h1 id="bird-detector-esp32-diy-guide">Bird detector esp32 DIY
guide</h1>
<h2 id="summary">Summary</h2>
<p>This is a complete guide to make a bird detector which runs entirely
on a low power esp32 microcontroller programmed with Arduino code. It
aims to make bioacoustic monitoring and embedded audio recognition more
accessible within creative coding and artscience communities.</p>
<p>The bird detector described in this guide consists of a
microcontroller which analyses a continuous audio stream from a
microphone and attempts to determine if a bird is vocalising in the
surrounding environment.</p>
<p>The bird detector is based on an embedded audio classifier which is
optimised to run on a low power microcontroller. The classifier is
trained on a custom dataset of bird vocalisations and environmental
sounds collected locally at the target deployment sites. The classifier
is trained and deployed using the Edge Impulse platform.</p>
<p>This guide presents a full pipeline, which is split in the following
sections:</p>
<ul>
<li>Data collection</li>
<li>Data preparation</li>
<li>Model training</li>
<li>Model deployment</li>
</ul>
<h3 id="data-collection">Data collection</h3>
<p>Data collection concerns recording a custom database of sound samples
that is used to train the audio classifier. A large number of samples is
needed for each target category. In our case we will work with two
categories: bird audio and non-bird audio. The audio is recorded in the
field at the same location, under similar conditions and with similar
instruments as during deployment. Data is gathered continuously over
time periods of multiple weeks using a Raspberry Pi running BirdNET-Pi
and a custom script to supplement the bird audio dataset with non-bird
audio.</p>
<h3 id="data-preparation">Data preparation</h3>
<p>Data preparation in this guide concerns preparing the data locally
before it is uploaded to Edge Impulse using custom python scripts. The
data is sorted, screened, segmented and reformatted in order to match
the parameters of the audio classifier.</p>
<h3 id="model-training">Model training</h3>
<p>Model training is performed using Edge Impulse platform. MFCCs are
extracted from the audio and used to train a lightweight neural network
which performs the classification.</p>
<h3 id="model-deployment">Model deployment</h3>
<p>For deployment Edge Impulse provides the audio classifier as an
Arduino library. This guide provides hardware schematics and code to
feed the classifier with a microphone signal. Two options are provided
for the microphone. The first option is a common MEMS microphone which
is low cost and easy to connect to the microcontroller, but of limited
quality and therefore detection accuracy. The second option is to
fabricate custom electronics using a high sensitivity electret
microphone in combination with a dedicated audio codec, which gives
better results.</p>
<h2 id="context-chorusing-symbionts-project">Context: Chorusing
Symbionts project</h2>
<p><img src="media/image8.jpg"
style="width:6.26772in;height:4.66667in" /></p>
<p>The bird detector described in this guide has been developed as part
of the project <a
href="https://matteomarangoni.com/Chorusing-Symbionts"><u>Chorusing
Symbionts</u></a>. This project is a sound art installation designed for
public parks and gardens. In this project a group of artificial
creatures generate music in real time in response to each other and the
environment. One of the project goals is to train artificial creatures
to recognise when birds are vocalising in their surroundings, so that
they can be programmed to respond through conflict avoidant
behaviours.</p>
<p>For the realisation of this project is was necessary to develop an
audio classifier with the following characteristics:</p>
<ul>
<li>All code runs locally on device, the constraints of embodied
artificial life require that computation runs on board without
connection to a remote server</li>
</ul>
<!-- -->
<ul>
<li><p>The classifier needs to run on an esp32 microcontroller using
Arduino code, for integration with existing sound generation and
behaviours developed in the previous project <a
href="https://matteomarangoni.com/Komorebi-page%5D"><u>Komorebi</u></a></p></li>
<li><p>The classifier must have low power requirements, it needs to run
on a small battery pack and a limited photovoltaic power source</p></li>
<li><p>The classifier needs to be low latency, to enable real time
musical responses</p></li>
<li><p>The classifier needs to recognise the majority of birds present
at the specific site where the artwork will be exhibited</p></li>
</ul>
<p>At the time of development (2023-2026), there was no existing off the
shelf solution to meet these requirements.</p>
<h2
id="context-existing-audio-classifiers-for-bird-recognition">Context:
existing audio classifiers for bird recognition</h2>
<p>Disclaimer: at the time of development of this project (2023-2026)
the field of embedded machine learning and bioacoustic monitoring is
evolving rapidly, therefore the information in this guide might be
outdated.</p>
<p>A commonly used classifier is <a
href="https://birdnet.cornell.edu/"><u>BirdNET</u></a>. This classifier
is trained to recognise over 6000 individual species, which is beyond
the requirements of this project.</p>
<p>However BirdNET currently does not provide a model that is able to
run on a microcontroller. Edge devices based on low power
microcontrollers such as an ESP32 are more commonly used to collect data
locally, which is then sent for analysis to a more powerful remote
server. This approach is ruled out by the project requirements, since
computation needs to be entirely on board.</p>
<p>An <a
href="https://github.com/birdnet-team/BirdNET-Tiny-Forge"><u>embedded
version</u></a> of BirdNET is still in early development and not useful
yet for the current application. <a
href="https://github.com/mcguirepr89/BirdNET-Pi"><u>BirdNET-PI</u></a>
is as close as a currently available solution comes, but it requires a
Raspberry Pi, which in terms of energy budget and purchase cost is an
order of magnitude higher compared to the ESP32.</p>
<p>Additionally, BirdNET works with 3 second chunks, which introduces an
amount of latency which is not desirable for a musical application.</p>
<p>Several research papers, such as <a
href="https://arxiv.org/abs/2407.21453"><u>Tiny Chirp</u></a> or the <a
href="https://biodcase.github.io/challenge2025/task3-results"><u>BioDCASE
2025 challenge 3 results</u></a>, present low power and low latency bird
audio classifiers with technical specifications compatible with project
requirements, but these typically focus on recognising a single bird
species. A further challenge is that these scientific papers do not
provide accessible instructions suitable for replication in the DIY
community. Furthermore they focus on the training of the classifier,
without covering aspects related to data collection or model deployment
which are covered in this guide.</p>
<h2 id="repo">Repo</h2>
<p>Github link <a
href="https://github.com/MatteoHMarangoni/bird_detector_esp32"><u>https://github.com/MatteoHMarangoni/bird_detector_esp32</u></a></p>
<p>Edge Impulse project <a
href="https://studio.edgeimpulse.com/public/806211/live"><u>https://studio.edgeimpulse.com/public/806211/live</u></a></p>
<h2 id="prerequisites">Prerequisites</h2>
<p>Here are listed hardware and software used for each of the pipeline
sections.</p>
<h3 id="data-collection-1">Data Collection</h3>
<p><a
href="https://www.raspberrypi.com/products/raspberry-pi-4-model-b/"><u>Raspberry
Pi 4B</u></a> + SD card, recommended 64GB or more, and a suitable power
supply</p>
<p><a
href="https://micbooster.com/product/clippy-em272-microphone/?v=1a13105b7e4e"><u>Clippy
EM272Z1 Mono Microphone</u></a> + <a
href="https://micbooster.com/product/puffer-urchin-clippy-windshield/?attribute_pa_colour=black&amp;v=1a13105b7e4e"><u>wind
jammer</u></a></p>
<p><a href="https://rode.com/en-nl/products/ai-micro"><u>Rode AI
Micro</u></a> audio interface</p>
<p><a
href="https://github.com/mcguirepr89/BirdNET-Pi"><u>BirdNET-Pi</u></a>
installed on the Raspberry Pi</p>
<h3 id="data-preparation-1">Data Preparation</h3>
<p><a href="https://github.com/birdnet-team/BirdNET-Analyzer"><u>BirdNET
Analyser</u></a> + dependencies</p>
<p><a href="https://github.com/snakers4/silero-vad"><u>Silero
VAD</u></a> for removal of human speech for privacy protection</p>
<p>Dependencies for running the data preparation scripts: - python =
"^3.11" - numpy = "^2.2.6" - scipy = "^1.15.3" - soundfile = "^0.13.1" -
sounddevice = "^0.5.2" - librosa = "^0.11.0" - python-osc = "^1.9.3" -
tqdm = "^4.67.1" - torch</p>
<h3 id="model-training-1">Model training</h3>
<p><a href="https://edgeimpulse.com/"><u>Edge Impulse</u></a> free
account is sufficient</p>
<h3 id="model-deployment-1">Model deployment</h3>
<p><a href="https://www.wemos.cc/en/latest/s3/s3_pro.html"><u>ESP32 S3
PRO</u></a> as microcontroller (other ESP32 boards should also work, but
might require some adjustments to the code and hardware
interfacing).</p>
<p><a href="http://l"><u>INMP441 MEMS microphone breakout
board</u></a></p>
<p>This microphone can be connected to the MCU simply with a breadboard
and a few jumper wires</p>
<p>We have made our own board using the <a
href="http://www.everest-semi.com/pdf/ES8388%20DS.pdf"><u>ES8388
Codec</u></a> and a CMC-4015-25T <a
href="http://mouser.com/ProductDetail/Same-Sky/CMC-4015-25T?qs=48I1WSKJTybY3H5TKh6cjA%3D%3D&amp;countryCode=DE&amp;currencyCode=EUR"><u>electret
microphone</u></a>. In the repo you can find the <a
href="https://github.com/MatteoHMarangoni/bird_detector_esp32/blob/main/electronics/custom_mic_schematic.png"><u>schematic</u></a>.</p>
<p><a href="https://platformio.org/"><u>PlatformIO</u></a> in <a
href="https://code.visualstudio.com/"><u>Visual Studio Code</u></a> for
code editing and firmware flashing, with “Espressif 32” platform and
“WEMOS LOLIN S3 PRO” board installed.</p>
<p>Dependencies to run deployment evaluation scripts - pyserial==3.5 -
pygame==2.5.2</p>
<h2 id="data-collection-2">Data Collection</h2>
<p>To train an audio classifier we need to first collect a large amount
of examples for the target categories. During a residency at TU
Eindhoven the advice was received to use own custom data, at least 5000
samples per target category, captured at the same location and with the
same type of equipment as during deployment.</p>
<p>Although our objective is to detect birds, we also need to train our
classifier on all kinds of other sounds that might be audible at the
site, so that the classifier will be able to distinguish between birds
and non-birds. Therefore in our case we have two target categories, bird
and non-bird. Non-bird audio is also referred to here as noise.</p>
<p>Given the amount of data and the need to create multiple sets and
classifiers for multiple exhibition sites, a semi-automated pipeline was
developed to collect data on site. BirdNET-Pi is used to automatically
sample and label bird audio data, while a custom script samples at
regular intervals environmental audio data for the noise category when
birds are not detected.</p>
<h3 id="hardware-setup">Hardware setup</h3>
<p>A <a
href="https://www.raspberrypi.com/products/raspberry-pi-4-model-b/"><u>Raspberry
Pi 4B</u></a> with a SD card and a suitable power supply is connected to
a <a href="https://rode.com/en-nl/products/ai-micro"><u>Rode AI
Micro</u></a> audio interface which receives a signal from a <a
href="https://micbooster.com/product/clippy-em272-microphone/?v=1a13105b7e4e"><u>Clippy
EM272Z1 Mono Microphone</u></a> protected by a <a
href="https://micbooster.com/product/puffer-urchin-clippy-windshield/?attribute_pa_colour=black&amp;v=1a13105b7e4e"><u>wind
jammer</u></a>. A network cable is used to connect the Raspberry Pi to a
laptop to access it remotely via ssh in terminal and FTP via an FTP
client.</p>
<h3 id="installing-birdnet-pi">Installing BirdNET-Pi</h3>
<p>Follow the instructions on:</p>
<p><a
href="https://github.com/mcguirepr89/BirdNET-Pi/wiki/Installation-Guide"><u>https://github.com/mcguirepr89/BirdNET-Pi/wiki/Installation-Guide</u></a></p>
<p><img src="media/image1.png"
style="width:6.26772in;height:4.63889in" /></p>
<p>Important: remember to install the correct OS as indicated by
BirdnetPi, the latest version might not be compatible</p>
<p>Once you have BirdNET-Pi installed, connect to it via network cable
and log in via the web interface accessible from a web browser at <a
href="http://birdnetpi.local/"><u>http://birdnetpi.local/</u></a>.</p>
<p>Under tools: settings, set the date and time, local coordinates and
preferred confidence settings for the detections. Check that the
microphone is working well using the real time audio monitoring feature
under “Live Audio” in the top right corner. Verify that detections are
being reported correctly by playing a few labelled samples of bird
vocalisations.</p>
<h3 id="custom-script">Custom script</h3>
<p>In the repo under “data_collection_script” you can find <a
href="https://github.com/MatteoHMarangoni/bird_detector_esp32/blob/main/data_collection_script/"><u>custom_script_birdnet_pi.py</u></a></p>
<p>This script runs in parallel to birdNET and saves samples of
environmental sound when there are no bird detections, in addition to
saving bird samples when there are detections.</p>
<p>You can access the data on the Raspberry-Pi via FTP by connecting to
it via network cable and using an FTP client like <a
href="https://cyberduck.io/"><u>Cyberduck</u></a>. You can find the
local network address of the Raspberry-Pi via Birdnet-Pi’s web interface
under tools: system info: viewer: LAN IP.</p>
<p>Copy the script to your Raspberry-Pi, placing it in the folder
BirdNET-Pi/scripts.</p>
<p>Then <a
href="https://www.instructables.com/Raspberry-Pi-Launch-Python-script-on-startup/"><u>set
the script to run on boot</u></a>.</p>
<p>Important: in the script you need to adjust lines 695 and 696 to
match the folder structure on your own Raspberry Pi.</p>
<p>You might also like to adjust values in lines 702 “non bird interval”
(how often to save a sample), 703 “non bird record duration” (the length
of the samples) and 704 “non bird volume threshold” (the minimum
loudness threshold) depending on the environmental noise level, the rate
at which the non-bird data is being sampled, how much data is needed
etc.</p>
<p>The script will save bird and non-bird data in the folder Birds:
StreamedAudio. Check that you are collecting samples at a sufficient
rate, that the audio quality is sufficient and that they are being
classified correctly.</p>
<h3 id="notes-about-using-the-setup-in-the-field">Notes about using the
setup in the field</h3>
<p>4-8 weeks of continuous operation should be sufficient to collect
enough raw data for one target location. This will vary by season and
location, depending on the density of bird populations and how
frequently they vocalise. The SD card should be checked every few weeks
in case it is getting full, or just use a larger card such as 256 GB,
when regular checks are not convenient.</p>
<p>The data collection setup needs mains power and is not waterproof,
therefore it needs to be placed in a covered area with access to power.
The microphone needs to be outside where it can capture bird
vocalisations. We have setup our RPI indoors with the microphone exiting
an opening in a window or wall and hanging under the eaves, or hanging
in a covered terrace. Note that the microphone cable is unshielded and
prone to EM interference. Keep away from power lines, including the
power supply of the Raspberry PI. This was not always possible during
data collection, therefore in the published dataset electrical hum can
is present.</p>
<p>If collecting data in public space, regulations for privacy need to
be observed. This varies by country. It might be necessary to place a
sign indicating that audio data is being captured. Recording time can
limited to short samples. A filter to remove human speech from the data
can also be used, birdNET-PI provides one which can be effective for
eliminating human speech from bird data. This is not yet implemented in
the custom script for non-bird data. It is important not to publish data
that contains private speech.</p>
<h3 id="resulting-data">Resulting data</h3>
<p>The custom script saves the data in two folders: bird / nonbird, each
used to train one category in the classifier</p>
<p>- bird files are 3 sec long, 48khz, with the name being the date,
time and the predicted species, e.g:
2025-08-05-16/13/01_6.0_9.0_bird_Eurasian_Magpie.wav</p>
<p>- non-bird files are 15 sec long, 48khz, with the name being the
date, time and then nonbird, e.g: 2025-08-05-13/54/54_nonbird.wav</p>
<h2 id="dataset-preparation">Dataset preparation</h2>
<p>This section describes how the collected data is processed locally
before uploading it to the Edge Impulse platform. The purpose of the
preparation is to curate a dataset with appropriate content, size and
file format, in order to minimise training time while maximising model
accuracy and latency.</p>
<p>Our output will be a collection of 1 sec mono samples for both birds
and noise, filtered for quality and diversity and then downsampled to
16khz.</p>
<p>The 1 second duration was chosen to reduce detection latency,
compared to the 3 seconds used by BirdNET. The sample rate of 16khz is
the lowest sample rate which can accurately capture bird audio while
remaining compatible with the sampling rate and computing power
available on the ESP32.</p>
<p>Ideally we want a similar amount of bird and non-bird data.</p>
<h3
id="understanding-audio-data-in-a-machine-learning-world">Understanding
audio data in a machine learning world</h3>
<p>The amount of data collected is too large to be manually reviewed and
labelled, therefore we use automated scripts to process the data. This
sorting needs to be based on an understanding of the characteristics of
the data and the goals and limitations of the classifier. It is crucial
therefore to manually review a representative portion of the raw data by
monitoring it over a high quality audio monitoring system, such as
professional speakers or headphones. Hi-fi devices will also work, but
not laptop speakers.</p>
<p>For those coming to this field with experience in music production or
sound engineering, and who are used to manually editing audio, this
method of approaching sound as data to be processed in large batches
through machine listening algorithms will be very different from the
traditional approach of reviewing individual audio samples by ear.</p>
<h3 id="reviewing-the-raw-data-challenges-and-solutions">Reviewing the
raw data: challenges and solutions</h3>
<p><img src="media/image3.jpg"
style="width:6.65104in;height:2.66263in" /></p>
<p>When reviewing the raw data, we found several challenges, which have
been addressed in the dataset preparation.</p>
<ul>
<li><p>The raw data is not of sufficient quality, we need to filter out
the lower quality data. This includes data that is incorrectly
classified during collection, recordings that are too soft, or bird data
where the signal to noise ratio is very poor.</p></li>
<li><p>Target sources are not represented in the proportions needed for
training. Some sources are over-represented, others under-represented.
This demonstrates the tendency of current machine learning systems to
reinforce imbalances and biases present in the real world. A curated
selection of the raw data is needed to have a better balance.</p></li>
<li><p>The format of the data is not the most suitable. While Edge
Impulse can take care of the data formatting, including segmentation to
1 second and downsampling to 16kHz, it will process the data each single
time a model is retrained, which is inefficient, therefore it is best to
do it before uploading the data to the platform. Also leaving the
segmentation to Edge Impulse will create the problem that many 1 segment
chunks will then be mislabelled (see below).</p></li>
</ul>
<!-- -->
<ul>
<li><p>The bird data includes a lot of silence, which are typical when
short bird calls which can have a duration of tens of milliseconds, are
contained within a 3 second sample. When segmenting the labelled bird
data from 3 second samples to 1 second samples, a significant amount of
samples are produced that contain no bird data. Therefore these 1 second
samples need to be screened again with a classifier. Since Birdnet
Analyser does not perform accurately on 1 second samples, we reduce the
confidence threshold for this screening pass.</p></li>
<li><p>The data includes a lot of samples that are too soft, we can
easily filter out samples by amplitude threshold.</p></li>
<li><p>The data includes bird samples in which the signal to noise ratio
is very poor. For our application it is good to have a dataset in which
bird audio is present in combination with environmental sounds, as this
is representative of the actual microphone signal that will be fed to
the classifier during deployment. We want the classifier to be able to
detect birds vocalising at a distance while other environmental sounds
(wind, traffic etc) are also present.. A high quality bird sample in
sound engineering terms would require the bird to be close to the
microphone in a silent environment, which would be a rare occurrence in
real world applications. For this reason it is appropriate to keep data
in which the signal to noise ratio (SNR) is poor, where bird audio is
signal and other sounds are noise. However we keep SNR into account when
selecting data for species that are overrepresented in the raw
data.</p></li>
<li><p>The raw bird data is overrepresenting certain species and
under-representing others. We rebalance this manually by selecting a
target amount of 1800 1s samples for the 25 most common species detected
at the location. When discarding redundant data we seek to discard lower
quality samples (lower confidence, or lower amplitude), while preserving
seasonal variations on sets spanning longer time periods. Bird songs vs
calls incidence is affected by seasonality, but there is no automatic
labelling of calls vs songs using Birdnet Analyser, so they need to be
evaluated by ear. This is clear for example in the data collected for
Robins, whose calls differ significantly from songs both in audio
content and seasonal occurrence. For birds of which we lack sufficient
data, we seek to include additional data from other locations. At this
time a solution is not provided to obtain data for the less common
species present at the target location.</p></li>
<li><p>Similarly, the raw noise data overrepresents certain sources
(i.e. rain) and under-represents others (i.e. dogs barking). At this
time we have not developed a solid method for balancing noise sources.
Data was sorted heuristically, manually reviewing in time sequential
order selected samples and discarding batches of data assumed to be
redundant. For example, rain tends to dominate the dataset for several
hours at a time, therefore a period in which rain is present can be
manually filtered out by deleting batches of samples within the time
period in which rain is recorded, without having to review each
individual sample.</p></li>
</ul>
<h3 id="section"></h3>
<h3
id="setting-up-the-virtual-environment-to-run-the-python-scripts-for-data-preparation">Setting
up the virtual environment to run the python scripts for data
preparation</h3>
<p>We use python scripts running within a virtual environment to
automate the sorting and screening of the data.</p>
<p>Install Virtual Studio Code and the PlatformIO extension.</p>
<p>Download the project repository and open it in PlatformIO</p>
<p><a
href="https://github.com/MatteoHMarangoni/bird_detector_esp32"><u>https://github.com/MatteoHMarangoni/bird_detector_esp32</u></a></p>
<p>In terminal install python 3.11 with &gt; brew install
python@3.11</p>
<p>In VS code go to view: command palette: Python: Create Environment:
Venv.</p>
<p>Select python 3.11.</p>
<p>Select to install the dependencies listed under
“requirements.txt.</p>
<p>This will create a .venv folder in the project repo which should
include all the dependencies. It can take a few minutes.</p>
<p>Open the terminal in PlatformIO (terminal button below code editor).
This should activate the virtual environment automatically, Verify that
the prompt starts with (.venv).</p>
<p>In PlatformIO file explorer within the project repo, in the folder
dagaset_preparation_scripts you can now run python scripts by selecting
them and clicking the “run python script” button above the code
editor.</p>
<p>The scripts will process all data contained in the
“dataset_preparation_scripts: data_raw” folder and output it in a new
folder within “dataset_preparation_scripts”.</p>
<h3 id="preparing-the-bird-data">Preparing the bird data</h3>
<p>Focus on the most common species present at the planned exhibition
location. We selected the 25 most represented species in the raw data.
Goal: 30 minutes, or 1800 1sec chunks per bird species. At 16bit 48khz,
this is 173mb of data. For 25 species we are aiming for approximately
4Gb of curated data.</p>
<p>Species in the published curated dataset set are:</p>
<ul>
<li>Common Chaffinch</li>
<li>Eurasian Blackcap</li>
<li>Egyptian Goose</li>
<li>Rose Ringed Parakeet</li>
<li>Eurasian Coot</li>
<li>Eurasian Magpie</li>
<li>Mallard</li>
<li>Eurasian Wren</li>
<li>Eurasian Jackdaw</li>
<li>Eurasian Jay</li>
<li>Gadwall</li>
<li>Great Spotted Woodpecker</li>
<li>Pigeon &amp; Dove (combined)</li>
<li>Short Toed Treecreeper</li>
<li>Carrion Crow</li>
<li>Grey Heron</li>
<li>Common Kingfisher</li>
<li>Canada Goose</li>
<li>Common Chiffchaff</li>
<li>European robin</li>
<li>Eurasian Blue Tit</li>
<li>Eurasian Green Woodpecker</li>
<li>Seagulls (combined)</li>
<li>Eurasian Blackbird</li>
<li>Great Tit</li>
</ul>
<p>The first thing we want to do is remove the silences from the 3sec
bird samples, so we segment the files into 1 sec chunks and remove the
ones that don't contain bird audio.</p>
<p>BirdNET Analyser doesn't work as well with 1 sec samples, so we have
to lower the confidence level substantially so we don't discard too much
useful bird data. The problem with this is that when we set a lower
confidence threshold, species are more likely to be mislabelled. This is
not a big problem as we are concerned with detecting birds generally and
not the specific species, however the labelling needs to be taken into
account when evaluating the target amount of recordings per species in
the dataset. When segmenting the data, we can also choose to maintain
the species label of the original detection present in the 3 second
chunk, which is more likely to be accurate.</p>
<p>We first run the bird data through the script “<a
href="https://github.com/MatteoHMarangoni/bird_detector_esp32/blob/main/dataset_preparation_scripts/screen_sort_bird.py"><u>screen_sort_bird.py</u></a>”.</p>
<p>Don’t forget to change the longitude and latitude of the location you
recorded the data in the script. This is used for accurate inference by
BirdNet Analyser.</p>
<p>You could also raise min_conf to 0.2 if you have enough data (or even
higher) so there is less low confidence data in the output (low
confidence is a general indication of a bad quality recording, but not
always)</p>
<p>Filenames of output from script:<br />
(sorted into folders with the species name of ‘previous filename’)</p>
<ul>
<li>‘confidence(from 1s inference)’-‘species name(from 1s
inference)’-‘previous filename’</li>
</ul>
<p>Optionally pass the resulting data through the script “<a
href="https://github.com/MatteoHMarangoni/bird_detector_esp32/blob/main/dataset_preparation_scripts/screen_sort_bird.py"><u>remove_birds_by_name.py</u></a>”</p>
<p>This compares the two bird species inferences, the original one from
BirdNet during the data collection and the new bird species from the 1s
inference. If these two species are not the same, it will move the file
to a separate folder to discard it.<br />
This is done because during testing it was found that if the species
don’t match, there is a high likelihood that the recording is of low
quality.</p>
<h4 id="manual-review-and-balancing-of-the-bird-data">Manual review and
balancing of the bird data</h4>
<p>Review the output from this process</p>
<p>How much data do we have for each target species?</p>
<p>Here is a table for the bird data gathered for the project in 2025 at
different locations (in mb).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"><strong>Bird</strong></th>
<th style="text-align: left;"><strong>Zuiderpark</strong></th>
<th style="text-align: left;"><strong>Marres</strong></th>
<th style="text-align: left;"><strong>Bronkhorst</strong></th>
<th style="text-align: left;"><strong>Amstelpark</strong></th>
<th style="text-align: left;"><strong>Zuidwal</strong></th>
<th style="text-align: left;"><strong>Combined</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Common Chiffchaff</td>
<td style="text-align: left;">30</td>
<td style="text-align: left;">32</td>
<td style="text-align: left;">805</td>
<td style="text-align: left;">3540</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">4407</td>
</tr>
<tr>
<td style="text-align: left;">Egyptian Goose</td>
<td style="text-align: left;">925</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">925</td>
</tr>
<tr>
<td style="text-align: left;">Rose Ringed Parakeet</td>
<td style="text-align: left;">810</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">20</td>
<td style="text-align: left;">26</td>
<td style="text-align: left;">856</td>
</tr>
<tr>
<td style="text-align: left;">Common Chaffinch</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">173.9</td>
<td style="text-align: left;">480.2</td>
<td style="text-align: left;">1.2</td>
<td style="text-align: left;">5.9</td>
<td style="text-align: left;">661.2</td>
</tr>
<tr>
<td style="text-align: left;">Eurasian Blackbird</td>
<td style="text-align: left;">1.9</td>
<td style="text-align: left;">193.4</td>
<td style="text-align: left;">81.3</td>
<td style="text-align: left;">347.4</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">624</td>
</tr>
<tr>
<td style="text-align: left;">Eurasian Coot</td>
<td style="text-align: left;">343</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">146</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">494</td>
</tr>
<tr>
<td style="text-align: left;">Eurasian Magpie</td>
<td style="text-align: left;">247</td>
<td style="text-align: left;">102.3</td>
<td style="text-align: left;">30</td>
<td style="text-align: left;">34</td>
<td style="text-align: left;">44</td>
<td style="text-align: left;">457.3</td>
</tr>
<tr>
<td style="text-align: left;">Eurasian Wren</td>
<td style="text-align: left;">134</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">125</td>
<td style="text-align: left;">171</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">430</td>
</tr>
<tr>
<td style="text-align: left;">Eurasian Blackcap</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">192.9</td>
<td style="text-align: left;">82.1</td>
<td style="text-align: left;">38.9</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">313.9</td>
</tr>
<tr>
<td style="text-align: left;">Pigeon &amp; Dove</td>
<td style="text-align: left;">62</td>
<td style="text-align: left;">185.2</td>
<td style="text-align: left;">39</td>
<td style="text-align: left;">19</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">305.2</td>
</tr>
<tr>
<td style="text-align: left;">European robin</td>
<td style="text-align: left;">28</td>
<td style="text-align: left;">192.1</td>
<td style="text-align: left;">11</td>
<td style="text-align: left;">48</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">283.1</td>
</tr>
<tr>
<td style="text-align: left;">Mallard</td>
<td style="text-align: left;">219</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">26</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">246</td>
</tr>
<tr>
<td style="text-align: left;">Eurasian Jackdaw</td>
<td style="text-align: left;">79</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">41</td>
<td style="text-align: left;">6</td>
<td style="text-align: left;">102</td>
<td style="text-align: left;">228</td>
</tr>
<tr>
<td style="text-align: left;">Great Spotted Woodpecker</td>
<td style="text-align: left;">71</td>
<td style="text-align: left;">96.5</td>
<td style="text-align: left;">8.9</td>
<td style="text-align: left;">15.1</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">191.5</td>
</tr>
<tr>
<td style="text-align: left;">Target amount</td>
<td style="text-align: left;">173</td>
<td style="text-align: left;">173</td>
<td style="text-align: left;">173</td>
<td style="text-align: left;">173</td>
<td style="text-align: left;">173</td>
<td style="text-align: left;">173</td>
</tr>
<tr>
<td style="text-align: left;">Carrion Crow</td>
<td style="text-align: left;">56</td>
<td style="text-align: left;">11.6</td>
<td style="text-align: left;">8</td>
<td style="text-align: left;">78</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">154.6</td>
</tr>
<tr>
<td style="text-align: left;">Short Toed Treecreeper</td>
<td style="text-align: left;">59</td>
<td style="text-align: left;">7.9</td>
<td style="text-align: left;">20</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">3.2</td>
<td style="text-align: left;">90.1</td>
</tr>
<tr>
<td style="text-align: left;">Eurasian Jay</td>
<td style="text-align: left;">79</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">81</td>
</tr>
<tr>
<td style="text-align: left;">Gadwall</td>
<td style="text-align: left;">72</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">73</td>
</tr>
<tr>
<td style="text-align: left;">Great Tit</td>
<td style="text-align: left;">1.3</td>
<td style="text-align: left;">43.5</td>
<td style="text-align: left;">14.9</td>
<td style="text-align: left;">1.4</td>
<td style="text-align: left;">7.5</td>
<td style="text-align: left;">68.6</td>
</tr>
<tr>
<td style="text-align: left;">Seagulls (combined)</td>
<td style="text-align: left;">4.1</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">63</td>
<td style="text-align: left;">68.1</td>
</tr>
<tr>
<td style="text-align: left;">Eurasian Green Woodpecker</td>
<td style="text-align: left;">6.2</td>
<td style="text-align: left;">26.5</td>
<td style="text-align: left;">12.3</td>
<td style="text-align: left;">9</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">54</td>
</tr>
<tr>
<td style="text-align: left;">Grey Heron</td>
<td style="text-align: left;">47</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">53</td>
</tr>
<tr>
<td style="text-align: left;">Common Kingfisher</td>
<td style="text-align: left;">47</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">5</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">52</td>
</tr>
<tr>
<td style="text-align: left;">Canada Goose</td>
<td style="text-align: left;">43</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">43</td>
</tr>
<tr>
<td style="text-align: left;">Eurasian Bluetit</td>
<td style="text-align: left;">10</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">20</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">12</td>
<td style="text-align: left;">44</td>
</tr>
</tbody>
</table>
<p>When we have more data than needed for the target location, we
discard the lower quality data (lower confidence samples). If there are
categories of birds that have too little data at the target location, we
can look into other datasets from other locations and integrate bird
data from those sets to compensate. For several common species which
were observed at the target location, we chose to include the data
although it was below the target amount.</p>
<h4 id="downsample-bird-data-to-16khz">Downsample bird data to
16khz</h4>
<p>- use <a
href="https://github.com/MatteoHMarangoni/bird_detector_esp32/blob/main/dataset_preparation_scripts/downsample_dataset.py"><u>downsample_dataset.py</u></a></p>
<h3 id="preparing-the-non-bird-noise-data">Preparing the non-bird
(noise) data</h3>
<p>Goal:</p>
<p>- purge bird audio from the non-bird data - discard very soft
recordings - format for training to 1 second 16khz - extract a total of
around 4 Gb (similar to total bird data),</p>
<p>To remove misclassified audio from this data run <a
href="https://github.com/MatteoHMarangoni/bird_detector_esp32/blob/main/dataset_preparation_scripts/purge_birds_from_nonbirds.py"><u>purge_birds_from_nonbirds.py</u></a></p>
<p>We run a script that does inference with a low confidence level (0.1
or lower, depending on the amount of non-bird data present), so it is
aggressively filtering out anything that might be a bird.</p>
<p>Optionally: if there is too much noise data, filter by loudness
threshold with <a
href="https://github.com/MatteoHMarangoni/bird_detector_esp32/blob/main/dataset_preparation_scripts/"><u>split_by_loudness.py</u></a></p>
<h4 id="manual-review-of-the-non-bird-data">Manual review of the non
bird data</h4>
<p>Review a representative selection of the dataset, listen if there are
still birds in there and if the non-bird data is useful.</p>
<p>If there is too much data, discard things like night recordings where
nothing much is happening, or data categories that are too redundant
(too much of the same, such as rain)</p>
<p>the goal is to have a sufficient amount of data for different noise
categories, including:</p>
<ul>
<li>continuous background noise (city hum)</li>
<li>Human voices (see privacy)</li>
<li>people walking</li>
<li>traffic sounds</li>
<li>airplane sounds</li>
<li>wind</li>
<li>rain</li>
<li>children playing</li>
<li>construction noise, power tools</li>
<li>music</li>
</ul>
<h4 id="final-prep-non-bird-data">Final prep non-bird data</h4>
<ul>
<li><p>Segment the 3s chunks into 1s chunks with <a
href="https://github.com/MatteoHMarangoni/bird_detector_esp32/blob/main/dataset_preparation_scripts/"><u>split_into_1s_chunks.py</u></a></p></li>
<li><p>downsample from 48khz to 16khz with <a
href="https://github.com/MatteoHMarangoni/bird_detector_esp32/blob/main/dataset_preparation_scripts/downsample_dataset.py"><u>downsample_dataset.py</u></a></p></li>
</ul>
<h3 id="removing-human-voices-privacy">Removing human voices
(privacy)</h3>
<p>- if data is to be published: remove (most) intelligible human speech
from data with <a
href="https://github.com/MatteoHMarangoni/bird_detector_esp32/blob/main/dataset_preparation_scripts/remove_human_speech.py"><u>remove_human_speech.py</u></a></p>
<p>Note: Review output and adjust settings if intelligible speech that
could relate to identifiable individuals is present. Under Dutch law
(AVG/GDPR), voice recordings may constitute personal data if a natural
person can be identified. Incidental, non-identifiable background speech
in public environments is generally low risk. However, sustained and
intelligible conversations between individuals should be removed prior
to publication. Speech delivered in a public performance or event may be
less privacy-sensitive.</p>
<h2 id="model-training-on-edge-impulse">Model training on Edge
Impulse</h2>
<p>For training we use Edge Impulse, a platform focusing on Machine
Learning in IoT applications.</p>
<p>Setup a free account on <a href="https://www.edgeimpulse.com"><u>Edge
Impulse</u></a> and clone the project:</p>
<p><a
href="https://studio.edgeimpulse.com/public/806211/live"><u>https://studio.edgeimpulse.com/public/806211/live</u></a></p>
<p>The platform provides a <a
href="https://docs.edgeimpulse.com/tutorials/end-to-end/sound-recognition"><u>tutorial
on training audio classifiers</u></a>.</p>
<p>In data acquisition upload the two data folders as “birds” and as
“noise”.</p>
<p>Let the data be split automatically in train and test set.</p>
<p><img src="media/image2.jpg"
style="width:6.26772in;height:1.63889in" /></p>
<p>In create impulse select the parameters for the input data, feature
extraction (MFCC) and learning block (classifier).</p>
<p><img src="media/image4.jpg"
style="width:6.26772in;height:2.68056in" /></p>
<p>Then proceed to MFCC extraction</p>
<p><img src="media/image5.jpg"
style="width:6.26772in;height:5.41667in" /></p>
<p>And finally model training</p>
<p><img src="media/image7.jpg"
style="width:6.26772in;height:8.69444in" /></p>
<h2 id="section-1"><img src="media/image6.png"
style="width:6.26772in;height:7.86111in" /></h2>
<p>When working with a new dataset, it might be useful to use the
provided Ion Tuner feature to sweep the parameter space and optimise
setting for the ESP32 as target platform.</p>
<p>Note that accuracy and latency values reported on Edge Impulse are an
estimation that does not correspond to what is measured in actual
deployment.</p>
<p>Once the model is trained, in the deployment tab export the quantized
model as an Arduino library.</p>
<p>You may also try the model in your phone’s browser before testing it
on the ESP32.</p>
<h2 id="model-deployment-2">Model deployment</h2>
<p>To deploy the model the ESP32 needs to be interfaced with a
microphone input. This repo provides the code and two hardware
solutions.</p>
<h3 id="hardware">Hardware</h3>
<p>We used an <a
href="https://www.espressif.com/en/products/socs/esp32"><u>ESP32 S3 with
8MB psram</u></a>, the <a
href="https://www.wemos.cc/en/latest/s3/s3_pro.html"><u>Wemos Lolin S3
Pro</u></a> provides a suitable development board.</p>
<p>The MEMS microphone <a
href="https://invensense.tdk.com/wp-content/uploads/2015/02/INMP441.pdf">INMP441</a><br />
is a popular option in combination with the ESP32, and low-cost breakout
boards are easily obtainable.</p>
<p>However, we noticed that performance increased substantially when
switching to a custom circuit using a dedicated codec (<a
href="https://dl.radxa.com/rock2/docs/hw/ds/ES8388%20user%20Guide.pdf">ES8388</a>)
together with an electret microphone (<a
href="https://eu.mouser.com/datasheet/3/6118/1/cmc-4015-25t.pdf">CMC-4015-25T</a>).</p>
<p>We use the microphone and codec in full differential mode for the
best signal-to-noise ratio.</p>
<p>See our electronic schematics here:<br />
<a
href="https://github.com/MatteoHMarangoni/bird_detector_esp32/blob/main/electronics/custom_mic_schematic.png">Custom
microphone schematic</a></p>
<p>When using the INMP441 breakout board, wire it as follows:</p>
<table>
<thead>
<tr>
<th>INMP441</th>
<th>LOLIN S3 PRO</th>
</tr>
</thead>
<tbody>
<tr>
<td>VDD</td>
<td>3V3</td>
</tr>
<tr>
<td>GND</td>
<td>GND</td>
</tr>
<tr>
<td>SCK</td>
<td>GPIO6 (BCLK)</td>
</tr>
<tr>
<td>WS</td>
<td>GPIO5 (LRCLK)</td>
</tr>
<tr>
<td>SD</td>
<td>GPIO7 (DATA OUT)</td>
</tr>
<tr>
<td>L/R</td>
<td>GND (Left channel)</td>
</tr>
</tbody>
</table>
<p>You may connect an LED to indicate a positive detection: GPIO15 → LED
→ 100Ω resistor → GND</p>
<p><img src="media/image9.jpg"
style="width:6.26772in;height:4.61111in" /></p>
<h3 id="code-installation-and-use">Code installation and use</h3>
<p>Depending on your hardware, download from the project repo the branch
for the <a
href="https://github.com/MatteoHMarangoni/bird_detector_esp32/tree/main"><u>custom
mic circuit</u></a> or <a
href="https://github.com/MatteoHMarangoni/bird_detector_esp32/tree/main"><u>MEMS
mic</u></a>.</p>
<p>If you have not yet installed the VSC, PlatformIO and the virtual
environment to run python scripts, first follow the instructions at
Dataset Preparation: Setting up the virtual environment…</p>
<p>Open the project in PlatformIO in VSC.</p>
<p>In main.cpp, choose mode (uncomment one):</p>
<ul>
<li>Classifier mode &gt; continuos classification via serial / LED</li>
<li>Communication mode &gt; allows use of custom script record and play
samples to the local drive</li>
</ul>
<p>If set in classifier mode, simply upload to MCU and open the serial
monitor to see the inferences, or simply observe the LED (light on
signals bird detected).</p>
<p>If set in communication mode, run the script <a
href="https://github.com/MatteoHMarangoni/bird_detector_esp32/blob/main/deployment_script/Serial_Audio.py"><u>Serial_Audio.py</u></a></p>
<p>This script allows to:</p>
<ul>
<li>Record samples, either individually or continuously from the
microphone to local drive. This can be used to test mic input signal or
to collect data for a new set.</li>
<li>Measure deployed accuracy (see further)</li>
</ul>
<h4 id="how-to-add-your-own-model">How to add your own model</h4>
<p>To add a different model trained on Edge Impulse, unzip the file we
got after building the model in edge impulse. The resulting folder we
have to place in the <u>“lib”</u> folder in the project code.</p>
<p>In the main.cpp file in the project folder, we change the naming on
line 19:<br />
#include &lt;Bird_Detector_ESP32_inferencing.h&gt; - Change the name of
the .h file to the exact name of the folder you just put in the lib
folder</p>
<h3 id="measuring-actual-accuracy-latency-power">Measuring actual
accuracy, latency, power</h3>
<p>Edge Impulse provides an estimation of accuracy by running inferences
on the test set. Actual accuracy in the field will differ. The test set
is split off from the same dataset as the training set. Differences in
real life applications will emerge due to variations in sound sources,
environmental conditions and hardware limitations.</p>
<p>The device reported the following latency:</p>
<p>Feature ext: 5.73 ms | Inference: 48.60 ms</p>
<p>Current draw was measured witha USB amp meter at approx 150mA @ 3.3
volt. for 1.05 seconds (including sampling time) = approx 0.044mAh per
inference</p>
<h4 id="accuracy-testing-with-sound-recordings">Accuracy Testing with
sound recordings</h4>
<p>To measure accuracy during deployment the following setup is
arranged, which is intended to compare the effects of hardware
(microphone inputs) on accuracy.</p>
<p>The bird detector running is placed is a quiet room and connected by
USB serial to a computer, the computer’s audio output is connected to
professional (or hi-fi) speaker system. The computer will play back
samples from a test dataset over the speakers, the ESP32 will perform
inferences over the sound captured by the microphone, the inferences can
then be compared to the original label to determine accuracy.</p>
<p>With the firmware set to Communication Mode, we can use the <a
href="https://github.com/MatteoHMarangoni/bird_detector_esp32/blob/main/deployment_script/Serial_Audio.py"><u>Serial_Audio.py</u></a>
script to verify deployed inference accuracy.</p>
<p>We need to add the files we want to use for testing to the project
folder. We do this in the “<u>input”</u> folder. In the folder there are
two subfolders, bird and no_bird. We add bird recordings in the bird
folder and non_bird / noise recordings in the no_bird folder. We aim for
around 1h of total data (so 30 min for each category). We used a portion
of the testing data downloaded from Edge Impulse (note this was already
downsampled to 16khz, better would have been to use 48khz but since the
training/test split was done automatically on the platform we wanted to
avoid using training data - would have been better to split training and
testing before downsampling and uploading to EI.</p>
<p>When running the script, in the terminal you will be presented with
some options, selected by sending different letter keys. First choose
your serial port.</p>
<p>Type ‘c’, to calibrate the offset between starting a sound file and
playback. Sending the c will play a 1.5s 2khz frequency multiple times,
with different offsets from starting the playback to starting the
recording, selecting the offset with the best coverage.</p>
<p>After the offset is calibrated, we can type the letter ‘a’ to start
the process. This will run the automatic evaluation script. It will
cycle through the files in the input folder (remember to point the
script to the test data). It will play a file from the input folder over
your computer's speakers. While the computer is playing back the file,
the MCU will make a recording using the onboard microphone, run an
inference on it. Then it will save it in the recordings folder on the
computer. Files from the bird input folder will be saved in the
recordings/bird folder. The result of the inference will be written in
the file name as bird or nonbird. Files from the no_bird folder will be
saved in the recordings/no_bird folder with the same naming
convention.</p>
<p>After the script has cycled through all the files we can compare the
amount of correct inferences, by parsing files by name (bird/non-bird)
in each of the folders. In the bird folder files named bird are correct
inferences. In the non-bird folder files named non-bird are correct
inferences.</p>
<p>We can also have a look at the confidence levels of the inference.
0.000 - 0.500 means it's classified to be a non_bird, while confidence
0.500 - 1.000 means it is classified as a bird. (So if a file from the
non_bird input is classified as 0.511-bird, this means its not very
confident about it) .</p>
<p>Measured deployment accuracy is 82% when using custom electronics for
the microphone input and 64% when using a common MEMS microphone.</p>
<h2 id="evaluation-reflection-future-work">Evaluation / reflection /
future work</h2>
<p>The project started in 2023 with the question: is it possible for an
artificial creature based on a low power microcontroller to recognise
bird sounds in the wild? This project shows that this is not only
possible, but it can be done reliably and efficiently within the time
and budgetary constraints of an individual art project.</p>
<p>Several aspects of the pipeline could be developed further:</p>
<ul>
<li>Improve non-bird audio collection script, for more useful, better
quality / less quantity non-bird data</li>
<li>parse non-bird data by sub-category for better balance between
sources</li>
<li>Design a data collector that is solar power and weather proof</li>
<li>Collect data with the same microphone and audio codec as used in
deployment</li>
<li>Further optimise training settings</li>
<li>Investigate if it be worthwhile to develop a local model training
routine, to be autonomous from commercial cloud computing services</li>
<li>Measure bird detector accuracy in the field in real world
conditions</li>
<li>Develop on device model training to add new species on site during
deployment</li>
</ul>
<h2 id="credits">Credits</h2>
<p>This project was developed by Matteo Maragoni with support from the
Chorusing Symbionts project team: - Ahnjili Zhuparris, project advisor
for machine learning, development data collection script - Stephan Olde,
IoT developer, development deployment code - Matthijs Munnik, custom
electronics design - Niels Gräber, project intern, data preparation</p>
<p>The publication of this DIY Bird Detector is supported by V2,
Rotterdam, Microdosing A.I. residency.</p>
<p>Chorusing Symbionts is produced in collaboration with:</p>
<ul>
<li>Crossing Parallels, Delft</li>
<li>Instrument inventors initiative, The Hague</li>
<li>Zuiderparktheatre, The Hague</li>
<li>Marres, Maastricht</li>
<li>Zone2Source, Amsterdam</li>
<li>STUK, Leuven</li>
</ul>
<p>Scientific advice:</p>
<ul>
<li>TU Delft, Crossing Parallels</li>
<li>TU Eindhoven, Innovation Space, Eindhoven Artificial Intelligence
Systems Institute</li>
<li>Naturalis Biodiversity Center, Arise project, Leiden</li>
</ul>
<p>Funding:</p>
<ul>
<li>Creative Industries fund NL</li>
<li>Performing Arts Fund NL</li>
<li>Municipality of The Hague</li>
<li>Amarte fonds</li>
</ul>
<h3 id="section-2"></h3>
</body>
</html>
